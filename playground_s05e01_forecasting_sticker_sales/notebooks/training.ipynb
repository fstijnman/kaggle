{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c2b307-0043-4393-915c-ff90bec1cee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent  # or Path(__file__).parent.parent\n",
    "\n",
    "train_df = pd.read_csv(PROJECT_ROOT / \"data\" / \"train.csv\", parse_dates=[\"date\"])\n",
    "test_df = pd.read_csv(PROJECT_ROOT / \"data\" / \"test.csv\", parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02f9b0e-7fcf-4c2f-85e7-a89be08c59bf",
   "metadata": {},
   "source": [
    "# Imputing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43fc1638-fa1e-457c-b2a2-7bef4b50ffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_per_capita_df = pd.read_csv(PROJECT_ROOT / \"data\" / \"gdp_per_capita.csv\")\n",
    "\n",
    "years =  [\"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"]\n",
    "gdp_per_capita_filtered_df = gdp_per_capita_df.loc[gdp_per_capita_df[\"Country Name\"].isin(train_df[\"country\"].unique()), [\"Country Name\"] + years].set_index(\"Country Name\")\n",
    "gdp_per_capita_filtered_df[\"2010_ratio\"] = gdp_per_capita_filtered_df[\"2010\"] / gdp_per_capita_filtered_df.sum()[\"2010\"]\n",
    "for year in years:\n",
    "    gdp_per_capita_filtered_df[f\"{year}_ratio\"] = gdp_per_capita_filtered_df[year] / gdp_per_capita_filtered_df.sum()[year]\n",
    "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_df[[i+\"_ratio\" for i in years]]\n",
    "gdp_per_capita_filtered_ratios_df.columns = [int(i) for i in years]\n",
    "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_ratios_df.unstack().reset_index().rename(columns = {\"level_0\": \"year\", 0: \"ratio\", \"Country Name\": \"country\"})\n",
    "gdp_per_capita_filtered_ratios_df['year'] = pd.to_datetime(gdp_per_capita_filtered_ratios_df['year'], format='%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72517bd0-c38b-41f8-b39b-2b4efe8e244e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values remaining: 8871\n",
      "Missing values remaining: 2\n"
     ]
    }
   ],
   "source": [
    "gdp_per_capita_filtered_ratios_df[\"year\"] = gdp_per_capita_filtered_ratios_df[\"year\"].dt.year\n",
    "train_df_imputed = train_df.copy()\n",
    "print(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")\n",
    "\n",
    "train_df_imputed[\"year\"] = train_df_imputed[\"date\"].dt.year\n",
    "for year in train_df_imputed[\"year\"].unique():\n",
    "    # Impute Time Series 1 (Canada, Discount Stickers, Holographic Goose)\n",
    "    target_ratio = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df[\"country\"] == \"Norway\"), \"ratio\"].values[0] # Using Norway as should have the best precision\n",
    "    current_raito = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df[\"country\"] == \"Canada\"), \"ratio\"].values[0]\n",
    "    ratio_can = current_raito / target_ratio\n",
    "    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] * ratio_can).values\n",
    "    \n",
    "    # Impute Time Series 2 (Only Missing Values)\n",
    "    current_ts =  train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n",
    "    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_can).values\n",
    "\n",
    "    # Impute Time Series 3 (Only Missing Values)\n",
    "    current_ts =  train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n",
    "    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_can).values\n",
    "    \n",
    "    # Impute Time Series 4 (Kenya, Discount Stickers, Holographic Goose)\n",
    "    current_raito = gdp_per_capita_filtered_ratios_df.loc[(gdp_per_capita_filtered_ratios_df[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df[\"country\"] == \"Kenya\"), \"ratio\"].values[0]\n",
    "    ratio_ken = current_raito / target_ratio\n",
    "    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\")& (train_df_imputed[\"year\"] == year), \"num_sold\"] * ratio_ken).values\n",
    "\n",
    "    # Impute Time Series 5 (Only Missing Values)\n",
    "    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n",
    "    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n",
    "\n",
    "    # Impute Time Series 6 (Only Missing Values)\n",
    "    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n",
    "    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n",
    "\n",
    "    # Impute Time Series 7 (Only Missing Values)\n",
    "    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year)]\n",
    "    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n",
    "    \n",
    "print(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed2e8a91-49f2-4bb1-8958-ea20cb8f9996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values remaining: 0\n"
     ]
    }
   ],
   "source": [
    "missing_rows = train_df_imputed.loc[train_df_imputed[\"num_sold\"].isna()]\n",
    "\n",
    "train_df_imputed.loc[train_df_imputed[\"id\"] == 23719, \"num_sold\"] = 4\n",
    "train_df_imputed.loc[train_df_imputed[\"id\"] == 207003, \"num_sold\"] = 195\n",
    "\n",
    "print(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1472f285-0d85-4962-b323-90321485305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df_imputed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa1fc599-5854-4976-bce4-27c61cec7c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(columns=['year'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d37dfb-e231-4599-8fa0-09e0f6f37d66",
   "metadata": {},
   "source": [
    "### Seasonality patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3920d53-241d-481f-b0d8-c35d811dce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    logger.info(\"Adding features\")\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # basic time features\n",
    "    logger.info(\"Adding basic time features\")\n",
    "    df['day_of_year'] = df['date'].dt.dayofyear\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['year'] = df['date'].dt.year\n",
    "    \n",
    "    # week based\n",
    "    logger.info(\"Adding week based features\")\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5,6]).astype(int)\n",
    "    \n",
    "    # cyclical encoding\n",
    "    logger.info(\"Adding cyclical features\")\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    \n",
    "    df['quarter_sin'] = np.sin(2 * np.pi * df['quarter'] / 4)\n",
    "    df['quarter_cos'] = np.cos(2 * np.pi * df['quarter'] / 4)\n",
    "    \n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365) \n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
    "\n",
    "    df['day_sin4'] = np.sin(8 * np.pi * df['day_of_year'] / 365)\n",
    "    df['day_cos4'] = np.cos(8 * np.pi * df['day_of_year'] / 365)\n",
    "\n",
    "    df['day_sin3'] = np.sin(6 * np.pi * df['day_of_year'] / 365)\n",
    "    df['day_cos3'] = np.cos(6 * np.pi * df['day_of_year'] / 365)\n",
    "\n",
    "    df['day_sin2'] = np.sin(4 * np.pi * df['day_of_year'] / 365)\n",
    "    df['day_cos2'] = np.cos(4 * np.pi * df['day_of_year'] / 365)\n",
    "\n",
    "    df['day_sin1'] = np.sin(1 * np.pi * df['day_of_year'] / 365)\n",
    "    df['day_cos1'] = np.cos(1 * np.pi * df['day_of_year'] / 365)\n",
    "\n",
    "    df[\"important_dates\"] = df[\"day_of_year\"].apply(lambda x: x if x in [1,2,3,4,5,6,7,8,9,10,99,100,101,125,126,355,256,357,358,359,360,361,362,363,364,365] else 0)\n",
    "\n",
    "    # one hot encoding\n",
    "    logger.info(\"Adding one hot encoding features\")\n",
    "    df = pd.get_dummies(df, columns=['country', 'store', 'product'], drop_first=True, dtype=int)\n",
    "    \n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b493398a-37c8-42ac-a709-2e0ded0709e8",
   "metadata": {},
   "source": [
    "### Adding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7280bf2f-a679-4bde-a9b9-fef13b740e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-21 17:51:23.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mAdding features\u001b[0m\n",
      "\u001b[32m2025-01-21 17:51:23.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madd_features\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mAdding features\u001b[0m\n",
      "\u001b[32m2025-01-21 17:51:23.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madd_features\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mAdding basic time features\u001b[0m\n",
      "\u001b[32m2025-01-21 17:51:23.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madd_features\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mAdding week based features\u001b[0m\n",
      "\u001b[32m2025-01-21 17:51:23.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madd_features\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mAdding cyclical features\u001b[0m\n",
      "\u001b[32m2025-01-21 17:51:23.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madd_features\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mAdding one hot encoding features\u001b[0m\n",
      "\u001b[32m2025-01-21 17:51:23.459\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mAdded 29 columns\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Adding features\")\n",
    "original_cols = len(train_df.columns)\n",
    "train_df = add_features(df = train_df)\n",
    "trans_cols = len(train_df.columns)\n",
    "logger.info(f\"Added {trans_cols - original_cols} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e99b25-4841-47bc-8a50-77e7f8b65f25",
   "metadata": {},
   "source": [
    "# Training & validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6061ef8c-7dd7-4afd-befe-a29c5773a24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 0, 'learning_rate': 0.1}\n",
      "{'max_depth': 1, 'learning_rate': 0.1}\n",
      "{'max_depth': 2, 'learning_rate': 0.1}\n",
      "{'max_depth': 3, 'learning_rate': 0.1}\n",
      "{'max_depth': 4, 'learning_rate': 0.1}\n",
      "{'max_depth': 5, 'learning_rate': 0.1}\n",
      "{'max_depth': 6, 'learning_rate': 0.1}\n",
      "{'max_depth': 7, 'learning_rate': 0.1}\n",
      "{'max_depth': 8, 'learning_rate': 0.1}\n",
      "{'max_depth': 9, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "for params in params_to_try: print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a41471d0-3f8c-4a4b-b7a8-c1bad22b9362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-21 17:56:31.549\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mMAPE score: 0.2376084587449392\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# train-test split\n",
    "training_data = train_df[train_df['date'] < '2015-01-01']\n",
    "validation_data = train_df[train_df['date'] >= '2015-01-01']\n",
    "\n",
    "X_train = training_data.drop(['id', 'date', 'num_sold'], axis=1)\n",
    "y_train = np.log1p(training_data['num_sold'])\n",
    "\n",
    "X_val = validation_data.drop(['id', 'date', 'num_sold'], axis=1)\n",
    "y_val = validation_data['num_sold']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred_transformed = np.expm1(y_pred)\n",
    "\n",
    "mape_score = mean_absolute_percentage_error(y_true=y_val, y_pred=y_pred_transformed)\n",
    "\n",
    "logger.info(f\"MAPE score: {mape_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a714fe2b-ada1-406d-a256-f07880b3f257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-21 18:00:02.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mMAPE score with params {'max_depth': 8, 'learning_rate': 0.1, 'n_estimators': 300}: 0.13847399128344023\u001b[0m\n",
      "\u001b[32m2025-01-21 18:00:02.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m\n",
      "Best parameters found: {'max_depth': 8, 'learning_rate': 0.1, 'n_estimators': 300}\u001b[0m\n",
      "\u001b[32m2025-01-21 18:00:02.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBest MAPE score: 0.13847399128344023\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# train-test split\n",
    "training_data = train_df[train_df['date'] < '2015-01-01']\n",
    "validation_data = train_df[train_df['date'] >= '2015-01-01']\n",
    "\n",
    "X_train = training_data.drop(['id', 'date', 'num_sold'], axis=1)\n",
    "y_train = np.log1p(training_data['num_sold'])\n",
    "\n",
    "X_val = validation_data.drop(['id', 'date', 'num_sold'], axis=1)\n",
    "y_val = validation_data['num_sold']\n",
    "\n",
    "params_to_try = [\n",
    "        {'max_depth': 8, 'learning_rate': 0.1, 'n_estimators': 300}\n",
    "    ]\n",
    "    \n",
    "best_mape = float('inf')\n",
    "best_params = None\n",
    "\n",
    "for params in params_to_try:\n",
    "    model = xgb.XGBRegressor(**params, objective='reg:squarederror', random_state=42)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_transformed = np.expm1(y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_val, y_pred_transformed)\n",
    "    \n",
    "    logger.info(f\"MAPE score with params {params}: {mape}\")\n",
    "    \n",
    "    if mape < best_mape:\n",
    "        best_mape = mape\n",
    "        best_params = params\n",
    "\n",
    "logger.info(f\"\\nBest parameters found: {best_params}\")\n",
    "logger.info(f\"Best MAPE score: {best_mape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1517a88f-4dd6-4a17-ba18-3e4657344782",
   "metadata": {},
   "source": [
    "### Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e210c47c-1e1b-4d76-a051-20ceb92cbad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data['y_pred'] = y_pred_transformed\n",
    "validation_dates = validation_data['date'].unique()\n",
    "\n",
    "filter_product = ((validation_data['country_Singapore'] == 1) & (validation_data['product_Kaggle'] == 1) & (validation_data['store_Premium Sticker Mart'] == 1))\n",
    "\n",
    "y_true_vals = validation_data[filter_product]['num_sold']\n",
    "y_pred_vals = validation_data[filter_product]['y_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0096d6a8-a1f3-437b-84aa-d7ca7703e265",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(validation_dates, y_true_vals)\n",
    "plt.plot(validation_dates, y_pred_vals)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69025273-01a2-4a39-98f0-dabb88ef1285",
   "metadata": {},
   "source": [
    "### Final submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "698b586f-2c5e-464b-8141-b18a7717341f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-21 18:01:22.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madd_features\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mAdding features\u001b[0m\n",
      "\u001b[32m2025-01-21 18:01:22.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madd_features\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mAdding basic time features\u001b[0m\n",
      "\u001b[32m2025-01-21 18:01:22.204\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madd_features\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mAdding week based features\u001b[0m\n",
      "\u001b[32m2025-01-21 18:01:22.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madd_features\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mAdding cyclical features\u001b[0m\n",
      "\u001b[32m2025-01-21 18:01:22.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madd_features\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mAdding one hot encoding features\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_df = add_features(df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea5e9f47-88ab-42ff-a876-c27e25f6dc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(['id', 'date', 'num_sold'], axis=1)\n",
    "y_train = np.log1p(train_df['num_sold'])\n",
    "\n",
    "X_test = test_df.drop(['id', 'date'], axis=1)\n",
    "\n",
    "params = {'max_depth': 8, 'learning_rate': 0.1, 'n_estimators': 300}\n",
    "\n",
    "model = xgb.XGBRegressor(**params, objective='reg:squarederror', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_transformed = np.expm1(y_pred)\n",
    "test_df['num_sold'] = y_pred_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65d3bed1-ec62-456d-a988-9579fa20907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_target = PROJECT_ROOT / 'data/submission.csv'\n",
    "\n",
    "test_df[['id', 'num_sold']].to_csv(submission_target, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
